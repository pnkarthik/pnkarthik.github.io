{
  "collaborators": 
  {
    "current": [
      {
        "name": "Vincent Y. F. Tan",
        "websiteLink": "https://vyftan.github.io",
        "photo": "vincent.png"
      },
      {
        "name": "Ali Tajer",
        "websiteLink": "https://www.isg-rpi.com",
        "photo": "ali-tajer.jpg"
      },
      {
        "name": "Krishna Jagannathan",
        "websiteLink": "https://www.ee.iitm.ac.in/~krishnaj/",
        "photo": "krishna.jpg"
      },
      {
        "name": "Yeow Meng Chee",
        "websiteLink": "https://ymchee66.github.io/home/",
        "photo": "ymchee.jpg"
      },
      {
        "name": "Kota Srinivas Reddy",
        "websiteLink": "https://sites.google.com/view/srinivas-reddy-kota/home",
        "photo": "srinivas.jpg"
      },
      {
        "name": "Chen Zhirui",
        "websiteLink": "https://scholar.google.com/citations?user=MMxA2qAAAAAJ&hl=en",
        "photo": "zhirui.jpeg"
      },
      {
        "name": "Arpan Mukherjee",
        "websiteLink": "https://mukherjee-arpan.github.io",
        "photo": "arpan.jpeg"
      },
      {
        "name": "Bharati Kamakoti",
        "websiteLink": "https://borate267.github.io",
        "photo": "bharati.jpg"
      }
    ],
    "past": [
      {
        "name": "Rajesh Sundaresan",
        "websiteLink": "https://ece.iisc.ac.in/~rajeshs/",
        "photo": "rajesh.jpg"
      },
      {
        "name": "Nikhil Karamchandani",
        "websiteLink": "https://sites.google.com/site/nikhilkaram/",
        "photo": "nikhil.jpeg"
      },
      {
        "name": "Jayakrishnan Nair",
        "websiteLink": "https://www.ee.iitb.ac.in/~jayakrishnan.nair/",
        "photo": "JK.jpeg"
      },
      {
        "name": "Sarath Yasodharan",
        "websiteLink": "https://sarath-yasodharan.github.io",
        "photo": "sarath.jpg"
      },
      {
        "name": "Nihesh Rathod",
        "websiteLink": "https://www.linkedin.com/in/nihesh-rathod/",
        "photo": "nihesh.jpg"
      },
      {
        "name": "Ajeesh Sahadevan",
        "websiteLink": "https://www.linkedin.com/in/ajeesh-sahadevan/",
        "photo": "ajeesh.jpeg"
      },
      {
        "name": "Pratik Verma",
        "websiteLink": "https://www.linkedin.com/in/vpratik/",
        "photo": "pratik.jpg"
      },
      {
        "name": "Chandra R. Murthy",
        "websiteLink": "https://ece.iisc.ac.in/~cmurthy/doku.php?id=home",
        "photo": "chandra.jpg"
      },
      {
        "name": "Neelesh B. Mehta",
        "websiteLink": "https://ece.iisc.ac.in/~nextgenwrl/Neelesh.html",
        "photo": "neelesh.jpg"
      },
      {
        "name": "Geethu Joseph",
        "websiteLink": "https://sites.google.com/view/geethujoseph/home",
        "photo": "geethu.jpg"
      },
      {
        "name": "Raksha Ramakrishna",
        "websiteLink": "https://www.linkedin.com/in/raksha-ramakrishna-b4390374/?originalSubdomain=se",
        "photo": "raksha.jpg"
      }
    ]
  },
  "preprints": [
    {
      "id": "icml2024",
      "title": "<em>Optimal Multi-Objective Best Arm Identification with Fixed Confidence</em>",
      "authors": "<a href=\"https://scholar.google.com/citations?user=MMxA2qAAAAAJ&hl=en\" target=\"_blank\"  rel=\"noopener noreferrer\">Chen Zhirui</a>, <a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://ymchee66.github.io/home/\" target=\"_blank\"  rel=\"noopener noreferrer\">Yeow Meng Chee</a>, and <a href=\"https://vyftan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>",
      "status":"Submitted, Feb 2024",
      "hasAbstract": true,
      "abstract": "<p>We consider a multi-armed bandit setting with finitely many arms, in which each arm yields an $M$-dimensional vector reward upon selection. We assume that the reward of each dimension (a.k.a. <em>objective</em>) is generated independently of the others. The best arm of any given objective is the arm with the largest component of mean corresponding to the objective. The end goal is to identify the best arm of <em>every</em> objective in the shortest (expected) time subject to an upper bound on the probability of error (i.e., fixed-confidence regime). We establish a problem-dependent lower bound on the limiting growth rate of the expected stopping time, in the limit of vanishing error probabilities. This lower bound, we show, is characterised by a max-min optimisation problem that is computationally expensive to solve at each time step. We propose an algorithm that uses the novel idea of <em>surrogate proportions</em> to sample the arms at each time step, eliminating the need to solve the max-min optimisation problem at each step. We demonstrate theoretically that our algorithm is asymptotically optimal. In addition, we provide empirical evidence, utilising both synthetic and real-world datasets, to substantiate the efficiency of our algorithm.  While existing works on pure exploration with multi-objective multi-armed bandits predominantly focus on <em>Pareto front identification</em>, our work fills the gap in the literature by conducting a formal investigation of the multi-objective best arm identification problem.</p>"
    },
    {
      "id": "pp3",
      "title": "<em>Learning to Detect an Odd Restless Markov Arm with a Trembling Hand</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "status": "",
      "hasAbstract": true,
      "abstract": "<p>This paper studies the problem of finding an anomalous arm in a multi-armed bandit when (a) each arm is a finite-state Markov process, and (b) the arms are restless. Here, anomaly means that the transition probability matrix (TPM) of one of the arms (the odd arm) is different from the common TPM of each of the non-odd arms. The TPMs are unknown to a decision entity that wishes to find the index of the odd arm as quickly as possible, subject to an upper bound on the error probability. We derive a problem instance-specific asymptotic lower bound on the expected time required to find the odd arm index, where the asymptotics is as the error probability vanishes. Further, we devise a policy based on the principle of certainty equivalence, and demonstrate that under a continuous selection assumption and a certain regularity assumption on the TPMs, the policy achieves the lower bound arbitrarily closely. Thus, while the lower bound is shown for all problem instances, the upper bound is shown only for those problem instances satisfying the continuous selection and the regularity assumptions. Our achievability analysis is based on resolving the identifiability problem in the context of a certain lifted countable-state controlled Markov process.</p>",
      "badgesData": [
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2105.03603",
          "badgeDisplayName": "arxiv"
        }
      ]
    },
    {
      "id": "pp4",
      "title": "<em>Axiomatic Characterisation of Projection Rules: An Open Question</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "hasAbstract": true,
      "status": "",
      "abstract": "<p>This paper studies the close interplay between two commonly used approaches for describing projection rules: (a) function-minimisation approach, and (b) axiomatic approach. As one of the findings of this study, this paper reports an interesting connection that the topic of conservative vector fields has with the axiomatic approach for describing projection rules. This leads to a question on conservative vector fields that is of independent mathematical interest. While answers to this question are known only in a few instances, a general solution to this question is currently not available. Contrary to papers that present concrete results in connection with a selected problem, the main purpose of this paper is to bring to light the above mathematical question.</p>",
      "badgesData": [
        {
          "badgeName": "draft",
          "link": "../media/2020/05/CsiszarPaperISIT2019.pdf",
          "badgeDisplayName": "draft"
        }
      ]
    }
  ],
  "journals": [
    {
      "id": "transIT2023",
      "title": "<em>Optimal Best Arm Identification with Fixed Confidence in Restless Bandits</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://vyftan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>, <a href=\"https://mukherjee-arpan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Arpan Mukherjee</a>, and <a href=\"https://www.isg-rpi.com\" target=\"_blank\"  rel=\"noopener noreferrer\">Ali Tajer</a>",
      "status":"Accepted, IEEE Transactions on Information Theory, 2024+",
      "hasAbstract": true,
      "abstract": "<p>We study best arm identification in a <em>restless</em> multi-armed bandit setting with finitely many arms. The discrete-time data generated by each arm forms a homogeneous Markov chain taking values in a common, finite state space. The state transitions in each arm are captured by an <em>ergodic</em> transition probability matrix (TPM) that is a member of a single-parameter exponential family of TPMs. The real-valued parameters of the arm TPMs are <em>unknown</em> and belong to a given space. Given a function $f$ defined on the common state space of the arms, the goal is to determine the best arm---the arm with the largest average value of $f$ evaluated under the arm's stationary distribution---with the fewest number of samples, subject to an upper bound on the decision's error probability (i.e., the <em>fixed-confidence</em> regime). A lower bound on the growth rate of the expected stopping time is established in the asymptote of a vanishing error probability. Furthermore, a policy for best arm identification is proposed, and its expected stopping time is proved to have an asymptotic growth rate that matches the lower bound. It is demonstrated that tracking the long-term behavior of a certain Markov decision process and its state-action visitation proportions are the key ingredients in analyzing the converse and achievability bounds. It is shown that under every policy, the state-action visitation proportions satisfy a certain approximate flow conservation constraint and that these proportions match the optimal proportions dictated by the lower bound under any asymptotically optimal policy. The prior studies on best arm identification focus on <em>independent observations</em> from the arms, <em>rested</em> Markov arms, and restless Markov arms with <em>known</em> arm TPMs. In contrast, this work is the first to study best arm identification in restless bandits with unknown arm TPMs.</p>", 
      "badgesData": [
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2310.13393",
          "badgeDisplayName": "arxiv"
        }
      ]
    },
    {
      "id": "T-IT-fed-BAI-het",
      "title": "<em>Federated Best Arm Identification with Heterogeneous Clients</em>",
      "authors": "<a href=\"https://scholar.google.com/citations?user=MMxA2qAAAAAJ&hl=en\" target=\"_blank\"  rel=\"noopener noreferrer\">Chen Zhirui</a>, <a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://vyftan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>, and <a href=\"https://ymchee66.github.io/home/\" target=\"_blank\"  rel=\"noopener noreferrer\">Yeow Meng Chee</a>",
      "status":"IEEE Transactions on Information Theory, volume 70, number 6, pp. 4258-4279, 2024",
      "hasAbstract": true,
      "abstract": "<p>We study best arm identification in a federated multi-armed bandit setting with a central server and multiple clients, when each client has access to a <em> subset </em> of arms and each arm yields independent Gaussian observations. The <em> reward </em> from an arm at any given time is defined as the average of the observations generated at this time across all the clients that have access to the arm. The end goal is to identify the best arm (the arm with the largest mean reward) of each client with the least expected stopping time, subject to an upper bound on the error probability (i.e., the <em>fixed-confidence regime</em>). We provide a lower bound on the growth rate of the expected time to find the best arm of each client. Furthermore, we show that for any algorithm whose upper bound on the expected time to find the best arms matches with the lower bound up to a multiplicative constant, the ratio of any two consecutive communication time instants must be bounded, a result that is of independent interest. We then provide the first-known lower bound on the expected number of <em> communication rounds </em> required to find the best arms. We propose a novel algorithm based on the well-known <em> Track-and-Stop </em> strategy that communicates only at exponential time instants, and derive asymptotic upper bounds on its expected time to find the best arms and the expected number of communication rounds, where the asymptotics is one of vanishing error probabilities.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/10335712",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2210.07780",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "paper",
          "link": "../media/2024/06/Federated_Best_Arm_Identification_With_Heterogeneous_Clients.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "BPL",
      "title": "<em>The Bus Priority Lane In Bengaluru: A Study of its Effectiveness and Driver Stress</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://www.nihesh.com\" target=\"_blank\"  rel=\"noopener noreferrer\">Nihesh Rathod</a>, <a href=\"https://sarath-yasodharan.github.io/\" target=\"_blank\"  rel=\"noopener noreferrer\">Sarath Yasodharan</a>, Wilson Lobo, <a href=\"https://www.linkedin.com/in/ajeesh-sahadevan/?originalSubdomain=in\" target=\"_blank\"  rel=\"noopener noreferrer\">Ajeesh Sahadevan</a>, <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a> and <a href=\"https://www.linkedin.com/in/vpratik/\" target=\"_blank\"  rel=\"noopener noreferrer\">Pratik Verma</a>",
      "status":"Special Issue on Sustainable City Transportation in the Indian Subcontinent, Transport Policy, May 2023",
      "hasAbstract": true,
      "abstract": "<p>This paper studies the effectiveness of the bus priority lane (BPL) for public transport buses in the city of Bengaluru in south India. We use the travel times on the BPL corridor as a measure of the effectiveness of the BPL. We find that there is a significant improvement in the travel times after the introduction of the BPL; for the worst 10% of the travel times, we find an improvement between 4% and 28%. Our methodology involves extracting trips on the BPL and computing the travel times for these trips from a time series of GPS information. Our methodology is scalable and can be used to compute the travel times between any two given points in other similar studies. We supplement our results with a novel test (called the D-test) for comparing the levels of stressful driving in the following scenarios: (a) morning peak hours (IST 07:00 hrs to 11:00 hrs) versus evening peak hours (IST 17:00 hrs to 21:00 hrs), and (b) northward trips versus southward trips on the BPL. We are able to infer that the drivers are generally more stressed during the morning peak hours and during the southward trips on the BPL. Partitioning the BPL into segments, we show that a majority of the segments exhibit similar effectiveness and driver stress trends as the full BPL stretch. We anticipate that corrective measures for the betterment of travel times and driver stress levels (e.g., introducing additional buses subject to vehicle re-balancing constraints, carefully planning the bus schedules to regulate bus traffic throughout the day, etc.) in some segments can lead to further improvements in travel times and reduction in driver stress levels.</p>",
      "badgesData": [
        {
          "badgeName": "arxiv",
          "link": "https://www.sciencedirect.com/science/article/abs/pii/S0967070X2300121X",
          "badgeDisplayName": "elsevier"
        },
        {
          "badgeName": "paper",
          "link": "../media/2023/05/BPL-paper.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "restless-bai",
      "title": "<em>Best Arm Identification in Restless Markov Multi-Armed Bandits</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://sites.google.com/view/srinivas-reddy-kota/home\" target=\"_blank\"  rel=\"noopener noreferrer\">Kota Srinivas Reddy</a>, and <a href=\"https://vyftan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>",
      "status":"IEEE Transactions on Information Theory, volume 69, number 5, pp. 3240-3262, 2023",
      "hasAbstract": true,
      "abstract": "<p>We study the problem of identifying the best arm in a multi-armed bandit environment when each arm is a time-homogeneous and ergodic discrete-time Markov process on a common, finite state space. The state evolution on each arm is governed by the arm's transition probability matrix (TPM). A decision entity that knows the set of arm TPMs but not the exact mapping of the TPMs to the arms, wishes to find the index of the best arm as quickly as possible, subject to an upper bound on the error probability. The decision entity selects one arm at a time sequentially, and all the unselected arms continue to undergo state evolution <em>restless</em> arms). For this problem, we derive the first-known problem instance-dependent asymptotic lower bound on the growth rate of the expected time required to find the index of the best arm, where the asymptotics is as the error probability vanishes. Further, we propose a sequential policy that, for an input parameter $R$, forcibly selects an arm that has not been selected for $R$ consecutive time instants. We show that this policy achieves an upper bound that depends on $R$ and is monotonically non-increasing as $R\\to\\infty$. The question of whether, in general, the limiting value of the upper bound as $R\\to\\infty$ matches with the lower bound, remains open. We identify a special case in which the upper and the lower bounds match. Prior works on best arm identification have dealt with (a) independent and identically distributed observations from the arms, and (b) rested Markov arms, whereas our work deals with the more difficult setting of restless Markov arms.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/9994751",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2203.15236",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "paper",
          "link": "../media/2023/05/Restless-BAI-known-TPMs.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "j1",
      "title": "<em>Detecting an Odd Restless Markov Arm with a Trembling Hand</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "status":"IEEE Transactions on Information Theory, volume 67, number 8, pp. 5230-5258, 2021",
      "hasAbstract": true,
      "abstract": "<p>In this paper, we consider a multi-armed bandit in which each arm is a Markov process evolving on a finite state space. The state space is common across the arms, and the arms are independent of each other. The transition probability matrix of one of the arms (the odd arm) is different from the common transition probability matrix of all the other arms. A decision maker, who knows these transition probability matrices, wishes to identify the odd arm as quickly as possible, while keeping the probability of decision error small. To do so, the decision maker collects observations from the arms by pulling the arms in a sequential manner, one at each discrete time instant. However, the decision maker has a trembling hand, and the arm that is actually pulled at any given time differs, with a small probability, from the one he intended to pull. The observation at any given time is the arm that is actually pulled and its current state. The Markov processes of the unobserved arms continue to evolve. This makes the arms restless. For the above setting, we derive the first known asymptotic lower bound on the expected time required to identify the odd arm, where the asymptotics is of vanishing error probability. The continued evolution of each arm adds a new dimension to the problem, leading to a family of Markov decision problems (MDPs) on a countable state space. We then stitch together certain parameterised solutions to these MDPs and obtain a sequence of strategies whose expected times to identify the odd arm come arbitrarily close to the lower bound in the regime of vanishing error probability. Prior works dealt with independent and identically distributed (across time) arms and rested Markov arms, whereas our work deals with restless Markov arms.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/9410612",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2005.06255",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "paper",
          "link": "../media/2021/08/Detecting_an_Odd_Restless_Markov_Arm_With_a_Trembling_Hand.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "j2",
      "title": "<em>Learning to Detect an Odd Markov Arm</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "status":"IEEE Transactions on Information Theory, volume 66, number 7, pp. 4324-4348, 2020",
      "hasAbstract": true,
      "abstract": "<p>A multi-armed bandit with finitely many arms is studied when each arm is a homogeneous Markov process on an underlying finite state space. The transition law of one of the arms, referred to as the odd arm, is different from the common transition law of all other arms. A learner, who has no knowledge of the above transition laws, has to devise a sequential test to identify the index of the odd arm as quickly as possible, subject to an upper bound on the probability of error. For this problem, we derive an asymptotic lower bound on the expected stopping time of any sequential test of the learner, where the asymptotics is as the probability of error vanishes. Furthermore, we propose a sequential test, and show that the asymptotic behaviour of its expected stopping time comes arbitrarily close to that of the lower bound. Prior works deal with independent and identically distributed arms, whereas our work deals with Markov arms. Our analysis of the rested Markov setting is a key first step in understanding the difficult case of restless Markov setting, which is still open.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/8990074",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/1904.11361",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "paper",
          "link": "../media/2020/07/Learning_to_Detect_an_Odd_Markov_Arm.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    }
  ],
  "conferencePublications": [
    {
      "id": "isit2024",
      "title": "<em>Best Arm Identification with Arm Erasures</em>",
      "authors":"<a href=\"https://sites.google.com/view/srinivas-reddy-kota/home\" target=\"_blank\" rel=\"noopener noreferrer\">Kota Srinivas Reddy</a>, <a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, and <a href=\"https://vyftan.github.io/\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>",
      "status":"IEEE International Symposium on Information Theory (<a href=\"https://2024.ieee-isit.org/home\" target=\"_blank\"  rel=\"noopener noreferrer\">ISIT 2024</a>), Jul 2024",
      "hasAbstract": true,
      "abstract": "<p>In this paper, we address the problem of best arm identification (BAI) with arm erasures in a multi-armed bandit setting with finitely many arms. A <em>learner</em> who seeks to identify the best arm---the arm with the largest mean reward---samples arms sequentially, one at each time instant, and communicates the sampled arm to an <em>agent</em> through an erasure channel with a known erasure probability $\\varepsilon \\in (0,1)$. The learner <em>does not</em> receive any erasure feedback, and hence does not know whether the transmitted arm was erased by the channel. In instances where erasure does not occur, and the transmitted arm is successfully received by the agent, the agent promptly pulls the received arm. On the contrary, when erasure occurs, we analyse the following two distinct scenarios: (a) the agent randomly selects an arm, and (b) the agent selects the most recent successfully received arm. We assume that the instantaneous reward from the pulled arm is available to the learner, whose objective is to find the best arm as quickly as possible, subject to an upper bound on the error probability. Given $\\delta \\in (0,1)$, we derive a problem-dependent lower bound on the expected stopping time of any algorithm whose error probability is within $\\delta$. We also propose two successive elimination algorithms for each of the aforementioned scenarios (a), (b), and provide upper bounds on their stopping times that hold with probability $1-\\delta$. To our best knowledge, this is the first work on BAI with arm erasures.</p>"
    },
    {
      "id": "iclr2024",
      "title": "<em>Fixed-Budget Differentially Private Best Arm Identification</em>",
      "authors": "<a href=\"https://scholar.google.com/citations?user=MMxA2qAAAAAJ&hl=en\" target=\"_blank\"  rel=\"noopener noreferrer\">Chen Zhirui</a>, <a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://ymchee66.github.io/home/\" target=\"_blank\"  rel=\"noopener noreferrer\">Yeow Meng Chee</a>, and <a href=\"https://vyftan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>",
      "status":"International Conference on Learning Representations (<a href=\"https://iclr.cc\" target=\"_blank\"  rel=\"noopener noreferrer\">ICLR 2024</a>), May 2024",
      "hasAbstract": true,
      "abstract": "<p>We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter  $\\varepsilon>0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain $\\varepsilon$-<em>differential privacy</em> ($\\varepsilon$-DP) constraint. We construct a policy satisfying the $\\varepsilon$-DP constraint (called $\\textsc{DP-BAI}$) by proposing the principle of <em>maximum absolute determinants</em>, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\\varepsilon$, and (c) the problem complexity that is expressible as the sum of two terms, one characterising the complexity of standard fixed-budget BAI (without privacy constraints), and the other accounting for the $\\varepsilon$-DP constraint. Additionally, we present some auxiliary results that contribute to the derivation of the lower bound on the error probability. These results, we posit, may be of independent interest and could prove instrumental in proving lower bounds on error probabilities in several other bandit problems. Whereas prior works provide results for BAI in the fixed-budget regime without privacy constraints or in the fixed-confidence regime with privacy constraints, our work fills the gap in the literature by providing the results for BAI in the fixed-budget regime under the $\\varepsilon$-DP constraint.</p>",
      "badgesData": [
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2401.09073",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "arxiv",
          "link": "https://openreview.net/forum?id=vrE2fqAInO&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions)",
          "badgeDisplayName": "openreview"
        }
      ]
    },
    {
      "id": "isit2023",
      "title": "<em>Best Arm Identification in Bandits with Limited Precision Sampling</em>",
      "authors":"<a href=\"https://sites.google.com/view/srinivas-reddy-kota/home\" target=\"_blank\" rel=\"noopener noreferrer\">Kota Srinivas Reddy</a>, <a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://sites.google.com/site/nikhilkaram/\" target=\"_blank\"  rel=\"noopener noreferrer\">Nikhil Karamchandani</a>, and <a href=\"https://www.ee.iitb.ac.in/~jayakrishnan.nair/\" target=\"_blank\"  rel=\"noopener noreferrer\">Jayakrishnan Nair</a>",
      "status":"IEEE International Symposium on Information Theory (<a href=\"https://isit2023.org/\" target=\"_blank\"  rel=\"noopener noreferrer\">ISIT 2023</a>), Jun 2023",
      "hasAbstract": true,
      "abstract": "<p>We study best arm identification in a variant of the multi-armed bandit problem where the learner has limited precision in arm selection. The learner can only sample arms via certain exploration bundles, which we refer to as boxes. In particular, at each sampling epoch, the learner selects a box, which in turn causes an arm to get pulled as per a box-specific probability distribution. The pulled arm and its instantaneous reward are revealed to the learner, whose goal is to find the best arm by minimising the expected stopping time, subject to an upper bound on the error probability. We present an asymptotic lower bound on the expected stopping time, which holds as the error probability vanishes. We show that the optimal allocation suggested by the lower bound is, in general, non-unique and therefore challenging to track. We propose a modified tracking-based algorithm to handle non-unique optimal allocations, and demonstrate that it is asymptotically optimal. We also present non-asymptotic lower and upper bounds on the stopping time in the simpler setting when the arms accessible from one box do not overlap with those of others.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/10206610",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2305.06082",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "paper",
          "link": "../media/2023/06/isit2023-best.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "aaai2023",
      "title": "<em>Almost Cost-Free Communication in Federated Best Arm Identification</em>",
      "authors": "<a href=\"https://sites.google.com/view/srinivas-reddy-kota/home\" target=\"_blank\"  rel=\"noopener noreferrer\">Kota Srinivas Reddy</a>, <a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, and <a href=\"https://vyftan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>",
      "status": "37th AAAI Conference on Artificial Intelligence (<a href=\"https://aaai-23.aaai.org/\" target=\"_blank\"  rel=\"noopener noreferrer\">AAAI 2023</a>), Feb 2023",
      "hasAbstract": true,
      "abstract": "<p>We study the problem of best arm identification in a federated learning multi-armed bandit setup with a central server and multiple clients. Each client is associated with a multi-armed bandit in which each arm yields <em>i.i.d.</em> rewards following a Gaussian distribution with an unknown mean and known variance. The set of arms is assumed to be the same at all the  clients. We define two notions of best arm$-$local and global. The local best arm at a client is the arm with the largest mean among the arms local to the client, whereas the global best arm is the arm with the largest  average mean across all the clients. We assume that each client can only observe the rewards from its local arms  and thereby estimate its local best arm. The clients communicate with a central server on  uplinks that entail a cost of $C\\ge0$ units per usage per uplink. The global best arm is estimated at the server. The goal is to identify the local best arms and the global best arm with minimal total cost, defined as the sum of the total number of arm selections at all the clients and the total communication cost, subject to an upper bound on the error probability. We propose a novel algorithm $\\textsc{FedElim}$  that is based on successive elimination and communicates only in exponential time steps, and obtain a high probability  instance-dependent upper bound on its total cost. The key takeaway from our paper is that for any $C\\geq 0$ and error probabilities sufficiently small, the total number of arm selections (resp. the total cost) under $\\textsc{FedElim}$ is at most $2$ (resp. $3$) times the maximum total number of arm selections under its variant that communicates in every time step. Additionally, we show that the latter is optimal in expectation up to a constant factor, thereby demonstrating that communication is almost cost-free in $\\textsc{FedElim}$. We numerically validate the efficacy of  $\\textsc{FedElim}$ on two synthetic datasets and the MovieLens dataset.</p>",
      "badgesData": [
        {
          "badgeName": "AAAI",
          "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26010",
          "badgeDisplayName": "AAAI"
        },
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/2208.09215",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "paper",
          "link": "../media/2023/02/AAAI2023-almost.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "itw2022",
      "title": "<em>Best Restless Markov Arm Identification</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>,  <a href=\"https://sites.google.com/view/srinivas-reddy-kota/home\" target=\"_blank\"  rel=\"noopener noreferrer\">Kota Srinivas Reddy</a>, and <a href=\"https://vyftan.github.io\" target=\"_blank\"  rel=\"noopener noreferrer\">Vincent Y. F. Tan</a>",
      "status": "<bf>IEEE Information Theory Workshop (<a href=\"https://itw2022.in/\" target=\"_blank\"  rel=\"noopener noreferrer\">ITW 2022</a>), Nov 2022</bf>",
      "hasAbstract": true,
      "abstract": "<p>We study the problem of best arm identification in multi-armed bandits when each arm is an ergodic Markov process that evolves whether or not the arm is selected (<em>restless</em> arms). The evolution of each arm's Markov process is governed by its transition probability matrix (TPM). A decision entity that knows the set of arm TPMs but not the exact mapping of the TPMs to the arms, wishes to find the index of the best arm as quickly as possible, subject to an upper bound on the error probability. We derive an asymptotic lower bound on the expected time required to find the best arm, where the asymptotics is as the error probability vanishes. Also, we design a policy that, for an input parameter $R,$ forcibly selects an arm that has not been selected for $R$ consecutive time instants, and achieves an upper bound that is monotonically non-increasing in $R$. Showing that, in general, the lower bound and the limiting value of the upper bounds as $R \\to\\infty$ match, appears to be difficult and remains open. These bounds are, however, shown to match in the special case when the TPM of each arm has identical rows, i.e., the arms yield independent and identically distributed  observations.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/9965908",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "paper",
          "link": "../media/2022/10/1570803352.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "cp1",
      "title": "<em>Learning to Detect an Odd Restless Markov Arm</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "status":"<bf>IEEE International Symposium on Information Theory (<a href=\"https://2021.ieee-isit.org/\" target=\"_blank\"  rel=\"noopener noreferrer\">ISIT 2021</a>), Jul 2021</bf>",
      "hasAbstract": true,
      "abstract": "<p>This paper studies the problem of identifying an anomalous arm in a multi-armed bandit when each arm is a finite-state Markov process and the arms are restless. Here, anomaly means that the transition probability matrix (TPM) of one of the arms (the odd arm) is different from the common TPM of each of the non-odd arms. The TPMs are unknown to a decision entity that wishes to find the index of the odd arm as quickly as possible, subject to an upper bound on the error probability. We derive an asymptotic lower bound on the expected time required to find the odd arm index, where the asymptotics is as the error probability vanishes. Further, we devise a policy based on the principle of certainty equivalence, and demonstrate that under a continuous selection assumption and a regularity assumption on the TPMs, the policy achieves the lower bound asymptotically. Our achievability analysis is based on resolving the identifiability problem in the context of a certain countable-state controlled Markov process.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/9518083",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "paper",
          "link": "../media/2021/07/1583.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "cp2",
      "title": "<em>Detecting an Odd Restless Markov Arm with a Trembling Hand</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "status":"<bf>IEEE International Symposium on Information Theory (<a href=\"https://2020.ieee-isit.org/\" target=\"_blank\"  rel=\"noopener noreferrer\">ISIT 2020</a>), Jun 2020</bf>",
      "hasAbstract": true,
      "abstract": "<p>Consider a multi-armed bandit whose arms are independent Markov processes on a common underlying state space. The transition probability matrix of one of the arms (the odd arm) is different from the common transition probability matrix of all the other arms. The goal is to identify the odd arm as quickly as possible while keeping the probability of decision error small. We study the case of restless Markov observations and identify an asymptotic lower bound on the expected stopping time for a decision with vanishing error probability. We then propose a sequential test and show that the asymptotic behaviour of its expected stopping time comes arbitrarily close to that of the lower bound. Prior works dealt with iid arms and rested Markov arms, whereas our work deals with restless Markov arms.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/9174397",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "paper",
          "link": "../media/2020/06/0002813.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "cp3",
      "title": "<em>Learning to Detect an Odd Markov</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "status":"<bf>IEEE International Symposium on Information Theory (<a href=\"https://2019.ieee-isit.org/\" target=\"_blank\"  rel=\"noopener noreferrer\">ISIT 2019</a>), Jul 2019</bf>",
      "hasAbstract": true,
      "abstract": "<p>A multi-armed bandit with finitely many arms is studied when each arm is a homogeneous Markov process on an underlying finite state space. The transition law of one of the arms, referred to as the odd arm, is different from the common transition law of all other arms. A learner, who has no knowledge of the above transition laws, has to devise a sequential test to identify the index of the odd arm as quickly as possible, subject to an upper bound on the probability of error. For this problem, we derive an asymptotic lower bound on the expected stopping time of any sequential test of the learner, where the asymptotics is as the probability of error vanishes. Furthermore, we propose a sequential test, and show that the asymptotic behaviour of its expected stopping time comes arbitrarily close to that of the lower bound. Prior works deal with iid arms, whereas our work deals with Markov arms.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/8849807",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "paper",
          "link": "../media/2019/07/Learning_to_Detect_an_Odd_Markov_Arm.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "cp4",
      "title": "<em>On The Equivalence of Projections in Relative $\\alpha$-Entropy and Renyi Divergence</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a> and  <a href=\"https://ece.iisc.ac.in/~rajeshs/\" target=\"_blank\"  rel=\"noopener noreferrer\">Rajesh Sundaresan</a>",
      "status":"<bf>National Conference on Communications (<a href=\"https://people.iith.ac.in/ncc2018/\" target=\"_blank\"  rel=\"noopener noreferrer\">NCC 2018</a>)</bf>, Feb 2018",
      "hasAbstract": true,
      "abstract": "<p>The aim of this work is to establish that two recently published projection theorems, one dealing with a parametric generalization of relative entropy and another dealing with Rényi divergence, are equivalent under a correspondence on the space of probability measures. Further, we demonstrate that the associated “Pythagorean” theorems are equivalent under this correspondence. Finally, we apply Eguchi's method of obtaining Riemannian metrics from general divergence functions to show that the geometry arising from the above divergences are equivalent under the aforementioned correspondence.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/8599980",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "arxiv",
          "link": "https://arxiv.org/abs/1701.06347",
          "badgeDisplayName": "arxiv"
        },
        {
          "badgeName": "paper",
          "link": "../media/2018/02/On_The_Equivalence_of_Projections_in_Relative_-_Entropy_and_Rnyi_Divergence.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    },
    {
      "id": "cp5",
      "title": "<em>Model-Based Interference Cartography and Visualization</em>",
      "authors": "<a href=\"/\" rel=\"noopener noreferrer\" class=\"nameFontColor\"><strong>P. N. Karthik</strong></a>, <a href=\"https://scholar.google.com/citations?&user=8Dom1fMAAAAJ&view_op=list_works&authuser=1&sortby=pubdate\" target=\"_blank\"  rel=\"noopener noreferrer\">Raksha Ramakrishna</a>, <a href=\"https://sites.google.com/view/geethujoseph/home\" target=\"_blank\"  rel=\"noopener noreferrer\">Geethu Joseph</a>, <a href=\"https://ece.iisc.ac.in/~cmurthy/doku.php?id=home\" target=\"_blank\"  rel=\"noopener noreferrer\">Chandra R. Murthy</a>, Joyson Sebastian, and <a href=\"https://ece.iisc.ac.in/~nextgenwrl/Neelesh.html\" target=\"_blank\"  rel=\"noopener noreferrer\">Neelesh B. Mehta</a>",
      "status":"<bf>National Conference on Communications (<a href=\"https://www.iitg.ac.in/ncc2016/\" target=\"_blank\"  rel=\"noopener noreferrer\">NCC 2016</a>)</bf>, Mar 2016",
      "hasAbstract": true,
      "abstract": "<p>In this work, we present a tool to construct and visualize the spatio-temporal variations of power. A dataset of real-world power measurements is collected over a geographical area of interest. Relevant parameters of the environment such as the path loss exponent and the decorrelation time of the lognormal shadow fading are extracted from the dataset. Also, the average powers measured at a finite set of known locations are interpolated to obtain the average power distribution over the area. Using the parameters of the lognormal shadow fading, synthetic data with the same temporal behavior of the dataset is generated, and multiplied with the average power distribution. The resulting spatio-temporal power map is displayed on the screen through a graphical user interface developed in-house. The proposed approaches for interpolation and parameter extraction are validated using test datasets generated using the well-accepted modified Gudmundson model for the spatio-temporal correlation of lognormal shadow fading. We also undertake a comparative study of three different interpolation techniques: linear interpolation, inverse distance weighing and ordinary kriging. Further, we compare a model-based approach with a model-free approach for interpolation, and find that model-based ordinary kriging provides the best mean absolute percentage error performance.</p>",
      "badgesData": [
        {
          "badgeName": "xplore",
          "link": "https://ieeexplore.ieee.org/document/7561174",
          "badgeDisplayName": "xplore"
        },
        {
          "badgeName": "paper",
          "link": "../media/2016/03/Model-based_interference_cartography_and_visualization.pdf",
          "badgeDisplayName": "pdf"
        }
      ]
    }
  ]
}